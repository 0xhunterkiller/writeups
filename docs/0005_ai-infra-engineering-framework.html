<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AI Infra Engineering Framework - writeups</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-1518c7cf.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-d8b3b7c5.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">writeups</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="ai-infra-engineer-framework"><a class="header" href="#ai-infra-engineer-framework">AI Infra Engineer Framework</a></h1>
<h2 id="section-0-pre-requisites"><a class="header" href="#section-0-pre-requisites">Section 0: Pre-requisites</a></h2>
<h3 id="linux-internals--cgroups-namespaces-numa-file-descriptors"><a class="header" href="#linux-internals--cgroups-namespaces-numa-file-descriptors">Linux internals – cgroups, namespaces, NUMA, file descriptors</a></h3>
<h3 id="containers-deep--runtimes-cgroups-v2-device-passthrough"><a class="header" href="#containers-deep--runtimes-cgroups-v2-device-passthrough">Containers deep – runtimes, cgroups v2, device passthrough</a></h3>
<h3 id="kubernetes-internals--scheduler-eviction-pdbs-probes"><a class="header" href="#kubernetes-internals--scheduler-eviction-pdbs-probes">Kubernetes internals – scheduler, eviction, PDBs, probes</a></h3>
<h3 id="networking--tcp-basics-latency-vs-throughput-dns-load-balancing"><a class="header" href="#networking--tcp-basics-latency-vs-throughput-dns-load-balancing">Networking – TCP basics, latency vs throughput, DNS, load-balancing</a></h3>
<h3 id="infra-as-code--terraform-state-drift-lifecycle"><a class="header" href="#infra-as-code--terraform-state-drift-lifecycle">Infra as Code – Terraform, state, drift, lifecycle</a></h3>
<h3 id="on-call-mindset--debugging-live-systems-postmortems"><a class="header" href="#on-call-mindset--debugging-live-systems-postmortems">On-call mindset – debugging live systems, postmortems</a></h3>
<hr>
<h2 id="section-1-why-gpus-exist"><a class="header" href="#section-1-why-gpus-exist">Section 1: Why GPUs Exist</a></h2>
<h3 id="11-why-cpus-broke-and-gpus-emerged"><a class="header" href="#11-why-cpus-broke-and-gpus-emerged">1.1 Why CPUs broke and GPUs emerged</a></h3>
<ul>
<li>Explains the strengths and limits of CPUs as general-purpose, low-parallel compute.</li>
<li>Explains GPUs as throughput-oriented systems built for massive parallelism.</li>
<li>Shows how workload characteristics shifted toward parallel execution.</li>
<li>Establishes GPUs as a response to structural compute limits, not a “faster CPU”.</li>
</ul>
<h3 id="12-why-ai-workloads-require-gpus"><a class="header" href="#12-why-ai-workloads-require-gpus">1.2 Why AI workloads require GPUs</a></h3>
<ul>
<li>Explains the computational patterns common in AI workloads.</li>
<li>Shows why these patterns map poorly to CPUs but efficiently to GPUs.</li>
<li>Establishes GPUs as a requirement for practical AI, not an optimization.</li>
</ul>
<h3 id="13-why-traditional-infra-assumptions-break-once-gpus-are-introduced"><a class="header" href="#13-why-traditional-infra-assumptions-break-once-gpus-are-introduced">1.3 Why traditional infra assumptions break once GPUs are introduced</a></h3>
<ul>
<li>Explains why GPU resources are scarce, fixed, and expensive compared to CPUs.</li>
<li>Shows how retry-based, stateless, elastic infra patterns fail with GPUs.</li>
<li>Establishes the need for different scheduling, scaling, and failure handling.</li>
</ul>
<h3 id="14-activities"><a class="header" href="#14-activities">1.4 Activities</a></h3>
<ul>
<li>Analyze historical CPU scaling limits and explain why parallelism, not clock speed, became the bottleneck.</li>
<li>Explain how GPUs changed infra assumptions without referencing specific AI frameworks.</li>
<li>Design a hypothetical infra assuming GPUs are “faster CPUs” and document where it fails.</li>
<li>Compare CPU-first vs GPU-first system designs and identify failure modes.</li>
<li>Present: “Why GPUs Exist and Why Infra Must Change”</li>
</ul>
<hr>
<h2 id="section-2-gpu-resource-model-device-level"><a class="header" href="#section-2-gpu-resource-model-device-level">Section 2: GPU Resource Model (Device Level)</a></h2>
<h3 id="21-what-resources-a-gpu-exposes"><a class="header" href="#21-what-resources-a-gpu-exposes">2.1 What resources a GPU exposes</a></h3>
<ul>
<li>Explains a GPU as a set of constrained resources, not a single opaque unit.</li>
<li>Introduces VRAM, compute units, and memory bandwidth as independent limits.</li>
<li>Establishes what infra is truly allocating and running out of.</li>
</ul>
<h3 id="22-how-gpu-memory-allocation-works"><a class="header" href="#22-how-gpu-memory-allocation-works">2.2 How GPU memory allocation works</a></h3>
<ul>
<li>Explains how GPU memory is allocated and held by processes.</li>
<li>Introduces fragmentation as a first-order constraint.</li>
<li>Establishes why “free VRAM” does not guarantee new workloads can start.</li>
</ul>
<h3 id="23-what-gpu-utilization-actually-measures"><a class="header" href="#23-what-gpu-utilization-actually-measures">2.3 What GPU utilization actually measures</a></h3>
<ul>
<li>Explains what GPU utilization metrics represent.</li>
<li>Shows why high utilization can still mean poor performance.</li>
<li>Establishes why utilization alone is an unreliable signal.</li>
</ul>
<h3 id="24-why-gpus-are-not-preemptible-like-cpus"><a class="header" href="#24-why-gpus-are-not-preemptible-like-cpus">2.4 Why GPUs are not preemptible like CPUs</a></h3>
<ul>
<li>Explains why GPU workloads cannot be cheaply paused or rescheduled.</li>
<li>Shows how long-running GPU tasks block resources.</li>
<li>Establishes why retries and preemption behave differently on GPUs.</li>
</ul>
<h3 id="25-cpugpu-interaction--hidden-bottlenecks"><a class="header" href="#25-cpugpu-interaction--hidden-bottlenecks">2.5 CPU–GPU Interaction &amp; Hidden Bottlenecks</a></h3>
<ul>
<li>Explains how GPUs depend on CPUs for feeding data and launching work.</li>
<li>Shows why CPU saturation can leave GPUs underutilized.</li>
<li>Establishes CPU–GPU balance as a core performance constraint.</li>
</ul>
<h3 id="26-activities"><a class="header" href="#26-activities">2.6 Activities</a></h3>
<ul>
<li>Explain the GPU resource model purely in terms of constraints, not capabilities.</li>
<li>Analyze why VRAM fragmentation causes failures even when memory appears available.</li>
<li>Reproduce GPU memory pressure scenarios and document allocation behavior.</li>
<li>Design a monitoring system that relies only on utilization and explain why it misleads.</li>
<li>Present: “What a GPU Resource Actually Is”</li>
</ul>
<hr>
<h2 id="section-3-from-one-gpu-to-many-topology-basics"><a class="header" href="#section-3-from-one-gpu-to-many-topology-basics">Section 3: From One GPU to Many (Topology Basics)</a></h2>
<h3 id="31-why-multi-gpu-nodes-exist"><a class="header" href="#31-why-multi-gpu-nodes-exist">3.1 Why multi-GPU nodes exist</a></h3>
<ul>
<li>Explains why some workloads cannot fit or run efficiently on a single GPU.</li>
<li>Introduces scale-up (more GPUs per node) as a necessity, not an optimization.</li>
<li>Establishes multi-GPU nodes as a response to workload size and communication needs.</li>
</ul>
<h3 id="32-what-gpu-topology-means"><a class="header" href="#32-what-gpu-topology-means">3.2 What “GPU topology” means</a></h3>
<ul>
<li>Explains that GPUs inside a node are not equally connected.</li>
<li>Introduces the idea that communication paths affect performance.</li>
<li>Establishes topology as a property infra must account for, not the application.</li>
</ul>
<h3 id="33-pcie-vs-nvlink"><a class="header" href="#33-pcie-vs-nvlink">3.3 PCIe vs NVLink</a></h3>
<ul>
<li>Explains the difference between general-purpose and high-bandwidth GPU interconnects.</li>
<li>Shows why communication speed becomes a bottleneck at scale.</li>
<li>Establishes why hardware choices directly affect workload scalability.</li>
</ul>
<h3 id="34-why-some-multi-gpu-jobs-scale-and-others-dont"><a class="header" href="#34-why-some-multi-gpu-jobs-scale-and-others-dont">3.4 Why some multi-GPU jobs scale and others don’t</a></h3>
<ul>
<li>Explains why adding GPUs does not guarantee linear speedup.</li>
<li>Introduces communication overhead as a dominant limiting factor.</li>
<li>Establishes that infra topology can make or break multi-GPU performance.</li>
</ul>
<h3 id="35-distributed-training-internals--collective-communication"><a class="header" href="#35-distributed-training-internals--collective-communication">3.5 Distributed Training Internals &amp; Collective Communication</a></h3>
<ul>
<li>Explains how data is exchanged across GPUs during training.</li>
<li>Shows why communication patterns dominate performance at scale.</li>
<li>Establishes distributed training behavior as topology-dependent.</li>
</ul>
<h3 id="36-activities"><a class="header" href="#36-activities">3.6 Activities</a></h3>
<ul>
<li>Explain why multi-GPU systems are not linear extensions of single-GPU systems.</li>
<li>Analyze how topology influences communication cost and scaling limits.</li>
<li>Map GPU interconnect topology on a real or simulated node.</li>
<li>Compare scaling behavior across different GPU topologies.</li>
<li>Present: “Why Topology Limits Scale Before Compute”</li>
</ul>
<hr>
<h2 id="section-4-gpu-sharing-models"><a class="header" href="#section-4-gpu-sharing-models">Section 4: GPU Sharing Models</a></h2>
<h3 id="41-why-sharing-gpus-is-hard"><a class="header" href="#41-why-sharing-gpus-is-hard">4.1 Why sharing GPUs is hard</a></h3>
<ul>
<li>Explains why GPUs lack fine-grained, cheap isolation like CPUs.</li>
<li>Shows how memory, execution, and bandwidth are tightly coupled.</li>
<li>Establishes sharing as a trade-off problem, not a default win.</li>
</ul>
<h3 id="42-full-gpu-allocation-one-job-one-gpu"><a class="header" href="#42-full-gpu-allocation-one-job-one-gpu">4.2 Full GPU allocation (one job, one GPU)</a></h3>
<ul>
<li>Explains the simplest and safest sharing model.</li>
<li>Shows why it maximizes isolation and predictability.</li>
<li>Establishes the baseline against which other models are judged.</li>
</ul>
<h3 id="43-gpu-time-slicing-process-level-sharing"><a class="header" href="#43-gpu-time-slicing-process-level-sharing">4.3 GPU time-slicing (process-level sharing)</a></h3>
<ul>
<li>Explains how multiple processes share a GPU over time.</li>
<li>Shows where interference and performance variance come from.</li>
<li>Establishes why time-slicing improves utilization but reduces predictability.</li>
</ul>
<h3 id="44-mig-hardware-level-partitioning"><a class="header" href="#44-mig-hardware-level-partitioning">4.4 MIG (hardware-level partitioning)</a></h3>
<ul>
<li>Explains how GPUs can be split into hardware-isolated slices.</li>
<li>Shows what MIG guarantees and what it does not.</li>
<li>Establishes MIG as a middle ground between isolation and utilization.</li>
</ul>
<h3 id="45-trade-offs-isolation-vs-utilization-vs-complexity"><a class="header" href="#45-trade-offs-isolation-vs-utilization-vs-complexity">4.5 Trade-offs: isolation vs utilization vs complexity</a></h3>
<ul>
<li>Explains why no sharing model is universally “best”.</li>
<li>Shows how operational complexity grows with utilization goals.</li>
<li>Establishes sharing decisions as infra policy, not application choice.</li>
</ul>
<h3 id="46-activities"><a class="header" href="#46-activities">4.6 Activities</a></h3>
<ul>
<li>Explain why GPU sharing is fundamentally harder than CPU sharing.</li>
<li>Compare isolation guarantees across full GPU, time-slicing, and MIG.</li>
<li>Design a GPU sharing policy for mixed training and inference workloads.</li>
<li>Evaluate utilization gains versus operational complexity for each sharing model.</li>
<li>Present: “GPU Sharing as an Infra Trade-off”</li>
</ul>
<hr>
<h2 id="section-5-gpu-failure-modes--debugging"><a class="header" href="#section-5-gpu-failure-modes--debugging">Section 5: GPU Failure Modes &amp; Debugging</a></h2>
<h3 id="51-common-gpu-failure-classes"><a class="header" href="#51-common-gpu-failure-classes">5.1 Common GPU failure classes</a></h3>
<ul>
<li>Explains the broad categories of failures GPUs experience.</li>
<li>Separates memory, compute, driver, and interconnect issues.</li>
<li>Establishes a mental model for classifying GPU problems before debugging.</li>
</ul>
<h3 id="52-what-gpu-oom-actually-looks-like"><a class="header" href="#52-what-gpu-oom-actually-looks-like">5.2 What GPU OOM actually looks like</a></h3>
<ul>
<li>Explains how GPU out-of-memory failures occur.</li>
<li>Shows why GPU OOMs are abrupt and non-recoverable.</li>
<li>Establishes why partial progress or graceful degradation is rare.</li>
</ul>
<h3 id="53-xid-errors-ecc-errors-and-gpu-resets"><a class="header" href="#53-xid-errors-ecc-errors-and-gpu-resets">5.3 Xid errors, ECC errors, and GPU resets</a></h3>
<ul>
<li>Explains what hardware-level GPU errors represent.</li>
<li>Shows how GPUs report faults differently than CPUs.</li>
<li>Establishes why some errors poison a GPU until reset or replacement.</li>
</ul>
<h3 id="54-why-gpu-failures-are-noisy-sticky-and-hard-to-recover"><a class="header" href="#54-why-gpu-failures-are-noisy-sticky-and-hard-to-recover">5.4 Why GPU failures are noisy, sticky, and hard to recover</a></h3>
<ul>
<li>Explains why GPU failures often cascade across jobs.</li>
<li>Shows how errors persist beyond a single process.</li>
<li>Establishes why automated recovery is difficult.</li>
</ul>
<h3 id="55-what-signals-exist-for-detecting-gpu-health"><a class="header" href="#55-what-signals-exist-for-detecting-gpu-health">5.5 What signals exist for detecting GPU health</a></h3>
<ul>
<li>Explains what observability signals GPUs expose.</li>
<li>Shows the limits of logs and metrics for early detection.</li>
<li>Establishes why GPU health monitoring requires specialized signals.</li>
</ul>
<h3 id="56-activities"><a class="header" href="#56-activities">5.6 Activities</a></h3>
<ul>
<li>Classify GPU failures by root cause and persistence.</li>
<li>Analyze why GPU OOMs behave differently from CPU OOMs.</li>
<li>Simulate a poisoned GPU scenario and document recovery limitations.</li>
<li>Build a failure decision tree for GPU incidents.</li>
<li>Present: “Why GPU Failures Are Sticky and Expensive”</li>
</ul>
<hr>
<h2 id="section-6-gpu-scheduling--placement"><a class="header" href="#section-6-gpu-scheduling--placement">Section 6: GPU Scheduling &amp; Placement</a></h2>
<h3 id="61-why-gpu-scheduling-is-harder-than-cpu-scheduling"><a class="header" href="#61-why-gpu-scheduling-is-harder-than-cpu-scheduling">6.1 Why GPU scheduling is harder than CPU scheduling</a></h3>
<ul>
<li>Explains why GPUs cannot be treated as interchangeable, fungible resources.</li>
<li>Shows how placement mistakes lead to idle GPUs or failed jobs.</li>
<li>Establishes scheduling as a first-order concern in GPU infra.</li>
</ul>
<h3 id="62-how-gpus-enter-kubernetes"><a class="header" href="#62-how-gpus-enter-kubernetes">6.2 How GPUs enter Kubernetes</a></h3>
<ul>
<li>Explains how GPUs are exposed to the scheduler.</li>
<li>Shows that GPUs are not native K8s resources by default.</li>
<li>Establishes why GPU scheduling depends on additional integration layers.</li>
</ul>
<h3 id="63-node-selection-taints-and-tolerations-for-gpus"><a class="header" href="#63-node-selection-taints-and-tolerations-for-gpus">6.3 Node selection, taints, and tolerations for GPUs</a></h3>
<ul>
<li>Explains how GPU-capable nodes are separated from general-purpose nodes.</li>
<li>Shows how workloads are constrained to specific hardware.</li>
<li>Establishes placement control as an infra responsibility.</li>
</ul>
<h3 id="64-why-any-free-gpu-is-a-lie"><a class="header" href="#64-why-any-free-gpu-is-a-lie">6.4 Why “any free GPU” is a lie</a></h3>
<ul>
<li>Explains why GPU count alone is insufficient for correct placement.</li>
<li>Shows how topology, sharing mode, and memory shape matter.</li>
<li>Establishes why naive bin-packing fails for GPUs.</li>
</ul>
<h3 id="65-scheduling-trade-offs-that-impact-cost-and-reliability"><a class="header" href="#65-scheduling-trade-offs-that-impact-cost-and-reliability">6.5 Scheduling trade-offs that impact cost and reliability</a></h3>
<ul>
<li>Explains how scheduling decisions directly affect GPU utilization.</li>
<li>Shows the tension between throughput, fairness, and isolation.</li>
<li>Establishes scheduling as a cost and reliability lever, not just correctness.</li>
</ul>
<h3 id="66-organizational--policy-constraints-in-gpu-scheduling"><a class="header" href="#66-organizational--policy-constraints-in-gpu-scheduling">6.6 Organizational &amp; Policy Constraints in GPU Scheduling</a></h3>
<ul>
<li>Explains how quotas, priorities, and policies shape scheduling outcomes.</li>
<li>Shows why many “scheduler bugs” are actually policy decisions.</li>
<li>Establishes GPU scheduling as both a technical and organizational system.</li>
</ul>
<h3 id="67-activities"><a class="header" href="#67-activities">6.7 Activities</a></h3>
<ul>
<li>Explain why GPU scheduling cannot rely on naive bin-packing.</li>
<li>Analyze how placement decisions affect cost and reliability.</li>
<li>Design a GPU scheduling policy under competing workload demands.</li>
<li>Simulate scheduling outcomes under different policy constraints.</li>
<li>Present: “Why ‘Any Free GPU’ Is a Lie”</li>
</ul>
<hr>
<h2 id="section-7-ai-workload-execution-models"><a class="header" href="#section-7-ai-workload-execution-models">Section 7: AI Workload Execution Models</a></h2>
<h3 id="71-training-vs-inference-infra-impact"><a class="header" href="#71-training-vs-inference-infra-impact">7.1 Training vs inference (infra impact)</a></h3>
<ul>
<li>Explains how training and inference place fundamentally different demands on infra.</li>
<li>Shows why uptime, scaling, and failure tolerance differ between the two.</li>
<li>Establishes why they cannot be operated the same way.</li>
</ul>
<h3 id="72-batch-jobs-vs-long-running-services"><a class="header" href="#72-batch-jobs-vs-long-running-services">7.2 Batch jobs vs long-running services</a></h3>
<ul>
<li>Explains the execution patterns of finite vs continuous workloads.</li>
<li>Shows how scheduling, retries, and resource holding differ.</li>
<li>Establishes why infra must distinguish between job types.</li>
</ul>
<h3 id="73-why-checkpoints-exist-and-when-they-matter"><a class="header" href="#73-why-checkpoints-exist-and-when-they-matter">7.3 Why checkpoints exist and when they matter</a></h3>
<ul>
<li>Explains checkpoints as a response to long-running, expensive computation.</li>
<li>Shows how checkpoints change failure recovery and scheduling behavior.</li>
<li>Establishes checkpoints as an infra concern, not just an ML feature.</li>
</ul>
<h3 id="74-execution-phases-startup-warmup-steady-state-teardown"><a class="header" href="#74-execution-phases-startup-warmup-steady-state-teardown">7.4 Execution phases: startup, warmup, steady-state, teardown</a></h3>
<ul>
<li>Explains that GPU workloads have distinct lifecycle phases.</li>
<li>Shows why resource usage and failure risk vary by phase.</li>
<li>Establishes why infra behavior must adapt across phases.</li>
</ul>
<h3 id="75-what-job-completion-means-for-gpu-workloads"><a class="header" href="#75-what-job-completion-means-for-gpu-workloads">7.5 What “job completion” means for GPU workloads</a></h3>
<ul>
<li>Explains completion beyond “process exited successfully”.</li>
<li>Shows how partial progress, retries, and recovery factor in.</li>
<li>Establishes correctness and cost as part of completion semantics.</li>
</ul>
<h3 id="76-activities"><a class="header" href="#76-activities">7.6 Activities</a></h3>
<ul>
<li>Explain why training and inference require different execution semantics.</li>
<li>Analyze retry behavior across different workload types.</li>
<li>Design checkpointing strategies for long-running GPU jobs.</li>
<li>Compare batch jobs and long-running services under failure.</li>
<li>Present: “Execution Semantics of AI Workloads”</li>
</ul>
<hr>
<h2 id="section-8-data-storage--io-for-ai-workloads"><a class="header" href="#section-8-data-storage--io-for-ai-workloads">Section 8: Data, Storage &amp; I/O for AI Workloads</a></h2>
<h3 id="81-why-data-access-is-often-the-real-bottleneck"><a class="header" href="#81-why-data-access-is-often-the-real-bottleneck">8.1 Why data access is often the real bottleneck</a></h3>
<ul>
<li>Explains why GPUs frequently wait on data instead of compute.</li>
<li>Shows how poor I/O hides behind “slow model” symptoms.</li>
<li>Establishes data access as a primary limiter of GPU efficiency.</li>
</ul>
<h3 id="82-object-storage-vs-local-disks-vs-network-filesystems"><a class="header" href="#82-object-storage-vs-local-disks-vs-network-filesystems">8.2 Object storage vs local disks vs network filesystems</a></h3>
<ul>
<li>Explains the trade-offs between common storage backends.</li>
<li>Shows how latency, throughput, and consistency differ.</li>
<li>Establishes why storage choice must match workload behavior.</li>
</ul>
<h3 id="83-dataset-locality-and-gpu-utilization"><a class="header" href="#83-dataset-locality-and-gpu-utilization">8.3 Dataset locality and GPU utilization</a></h3>
<ul>
<li>Explains why where data lives matters as much as how fast GPUs are.</li>
<li>Shows how remote data access reduces effective GPU usage.</li>
<li>Establishes locality as an infra-level optimization lever.</li>
</ul>
<h3 id="84-checkpoints-artifacts-and-model-storage"><a class="header" href="#84-checkpoints-artifacts-and-model-storage">8.4 Checkpoints, artifacts, and model storage</a></h3>
<ul>
<li>Explains the different types of data produced and consumed by AI workloads.</li>
<li>Shows why durability and access patterns differ across them.</li>
<li>Establishes storage strategy as part of execution design.</li>
</ul>
<h3 id="85-why-fast-gpus--slow-io-wastes-money"><a class="header" href="#85-why-fast-gpus--slow-io-wastes-money">8.5 Why fast GPUs + slow I/O wastes money</a></h3>
<ul>
<li>Explains how I/O bottlenecks directly translate to GPU idle time.</li>
<li>Shows the cost impact of mismatched compute and storage.</li>
<li>Establishes balanced infra as a cost-control requirement.</li>
</ul>
<h3 id="86-activities"><a class="header" href="#86-activities">8.6 Activities</a></h3>
<ul>
<li>Explain why data access, not compute, often limits GPU efficiency.</li>
<li>Analyze how storage latency and throughput affect utilization.</li>
<li>Profile GPU idle time caused by slow data pipelines.</li>
<li>Redesign a data pipeline to improve locality and throughput.</li>
<li>Present: “Why Fast GPUs Don’t Matter With Slow I/O”</li>
</ul>
<hr>
<h2 id="section-9-inference-infrastructure--serving"><a class="header" href="#section-9-inference-infrastructure--serving">Section 9: Inference Infrastructure &amp; Serving</a></h2>
<h3 id="91-what-makes-inference-different-from-training"><a class="header" href="#91-what-makes-inference-different-from-training">9.1 What makes inference different from training</a></h3>
<ul>
<li>Explains why inference prioritizes latency and availability over throughput.</li>
<li>Shows how failure tolerance is lower than in training.</li>
<li>Establishes inference as a production-critical workload.</li>
</ul>
<h3 id="92-online-vs-batch-inference"><a class="header" href="#92-online-vs-batch-inference">9.2 Online vs batch inference</a></h3>
<ul>
<li>Explains synchronous vs asynchronous inference patterns.</li>
<li>Shows how scaling and resource usage differ.</li>
<li>Establishes why infra choices depend on access patterns.</li>
</ul>
<h3 id="93-common-inference-runtimes"><a class="header" href="#93-common-inference-runtimes">9.3 Common inference runtimes</a></h3>
<ul>
<li>Explains why specialized runtimes exist for serving models.</li>
<li>Shows how runtimes differ in memory use and batching behavior.</li>
<li>Establishes runtime choice as an infra decision.</li>
</ul>
<h3 id="94-runtime-trade-offs-latency-throughput-memory"><a class="header" href="#94-runtime-trade-offs-latency-throughput-memory">9.4 Runtime trade-offs: latency, throughput, memory</a></h3>
<ul>
<li>Explains the inherent trade-offs between serving goals.</li>
<li>Shows why optimizing one dimension hurts another.</li>
<li>Establishes tuning as an ongoing infra task.</li>
</ul>
<h3 id="95-model-loading-warmup-and-cold-starts"><a class="header" href="#95-model-loading-warmup-and-cold-starts">9.5 Model loading, warmup, and cold starts</a></h3>
<ul>
<li>Explains why models are expensive to load into memory.</li>
<li>Shows how cold starts impact latency and availability.</li>
<li>Establishes warmup as a production concern.</li>
</ul>
<h3 id="96-why-autoscaling-inference-is-harder-than-it-looks"><a class="header" href="#96-why-autoscaling-inference-is-harder-than-it-looks">9.6 Why autoscaling inference is harder than it looks</a></h3>
<ul>
<li>Explains why request rate alone is insufficient for scaling.</li>
<li>Shows how model memory and startup time constrain scaling.</li>
<li>Establishes careful autoscaling as critical for cost and reliability.</li>
</ul>
<h3 id="97-activities"><a class="header" href="#97-activities">9.7 Activities</a></h3>
<ul>
<li>Explain why inference prioritizes different infra guarantees than training.</li>
<li>Analyze failure tolerance differences between online and batch inference.</li>
<li>Design an inference system optimized for latency and cost.</li>
<li>Compare cold-start and warm-start serving strategies.</li>
<li>Present: “Running Inference Systems in Production”</li>
</ul>
<hr>
<h2 id="section-10-model-packaging--deployment-artifacts"><a class="header" href="#section-10-model-packaging--deployment-artifacts">Section 10: Model Packaging &amp; Deployment Artifacts</a></h2>
<h3 id="101-why-models-are-not-just-files"><a class="header" href="#101-why-models-are-not-just-files">10.1 Why models are not “just files”</a></h3>
<ul>
<li>Explains why models include code, configuration, and runtime assumptions.</li>
<li>Shows how incompatibilities surface at deployment time.</li>
<li>Establishes packaging as a source of failure if ignored.</li>
</ul>
<h3 id="102-model-formats-and-their-implications"><a class="header" href="#102-model-formats-and-their-implications">10.2 Model formats and their implications</a></h3>
<ul>
<li>Explains common model formats and what they encode.</li>
<li>Shows how formats affect portability and performance.</li>
<li>Establishes format choice as an infra constraint.</li>
</ul>
<h3 id="103-quantization-and-precision-choices"><a class="header" href="#103-quantization-and-precision-choices">10.3 Quantization and precision choices</a></h3>
<ul>
<li>Explains why models are deployed at different precisions.</li>
<li>Shows how precision affects memory, speed, and accuracy.</li>
<li>Establishes quantization as an infra-performance trade-off.</li>
</ul>
<h3 id="104-containerizing-models-and-runtimes"><a class="header" href="#104-containerizing-models-and-runtimes">10.4 Containerizing models and runtimes</a></h3>
<ul>
<li>Explains how models are bundled with serving infrastructure.</li>
<li>Shows why containers must align with drivers and runtimes.</li>
<li>Establishes container builds as part of deployment correctness.</li>
</ul>
<h3 id="105-versioning-rollbacks-and-compatibility"><a class="header" href="#105-versioning-rollbacks-and-compatibility">10.5 Versioning, rollbacks, and compatibility</a></h3>
<ul>
<li>Explains why model changes must be reversible.</li>
<li>Shows how mismatches cause silent failures.</li>
<li>Establishes disciplined rollout as an infra responsibility.</li>
</ul>
<h3 id="106-model-release-pipelines--cicd-for-ai"><a class="header" href="#106-model-release-pipelines--cicd-for-ai">10.6 Model Release Pipelines &amp; CI/CD for AI</a></h3>
<ul>
<li>Explains how models move from experimentation to production.</li>
<li>Shows why model rollout carries different risks than code rollout.</li>
<li>Establishes CI/CD as a safety mechanism for model deployment.</li>
</ul>
<h3 id="107-activities"><a class="header" href="#107-activities">10.7 Activities</a></h3>
<ul>
<li>Explain why models are deployment artifacts, not static files.</li>
<li>Analyze compatibility risks across model formats and runtimes.</li>
<li>Package the same model in multiple formats and compare behavior.</li>
<li>Design a rollback strategy for a breaking model change.</li>
<li>Present: “Model Packaging and Deployment Risk”</li>
</ul>
<hr>
<h2 id="section-11-security--multi-tenancy-for-ai-infra"><a class="header" href="#section-11-security--multi-tenancy-for-ai-infra">Section 11: Security &amp; Multi-Tenancy for AI Infra</a></h2>
<h3 id="111-gpu-isolation-boundaries"><a class="header" href="#111-gpu-isolation-boundaries">11.1 GPU isolation boundaries</a></h3>
<ul>
<li>Explains where isolation can and cannot be enforced.</li>
<li>Shows the limits of pod-, node-, and hardware-level isolation.</li>
<li>Establishes isolation as a layered concern.</li>
</ul>
<h3 id="112-secrets-and-access-control"><a class="header" href="#112-secrets-and-access-control">11.2 Secrets and access control</a></h3>
<ul>
<li>Explains why models and data require protection.</li>
<li>Shows how credentials leak through poor infra design.</li>
<li>Establishes access control as part of infra, not application code.</li>
</ul>
<h3 id="113-supply-chain-risks"><a class="header" href="#113-supply-chain-risks">11.3 Supply chain risks</a></h3>
<ul>
<li>Explains risks introduced by images, dependencies, and model weights.</li>
<li>Shows how trust boundaries are crossed during deployment.</li>
<li>Establishes supply chain security as an infra responsibility.</li>
</ul>
<h3 id="114-abuse-prevention-for-inference-endpoints"><a class="header" href="#114-abuse-prevention-for-inference-endpoints">11.4 Abuse prevention for inference endpoints</a></h3>
<ul>
<li>Explains how public inference can be misused.</li>
<li>Shows cost and availability risks from abuse.</li>
<li>Establishes guardrails as necessary for production exposure.</li>
</ul>
<h3 id="115-activities"><a class="header" href="#115-activities">11.5 Activities</a></h3>
<ul>
<li>Explain why GPU isolation is inherently imperfect.</li>
<li>Analyze security risks in multi-tenant GPU environments.</li>
<li>Design a security model for shared inference infrastructure.</li>
<li>Evaluate blast radius of a compromised model endpoint.</li>
<li>Present: “Security Boundaries in AI Infrastructure”</li>
</ul>
<hr>
<h2 id="section-12-gpu-cluster-provisioning--lifecycle"><a class="header" href="#section-12-gpu-cluster-provisioning--lifecycle">Section 12: GPU Cluster Provisioning &amp; Lifecycle</a></h2>
<h3 id="121-how-gpu-nodes-are-provisioned"><a class="header" href="#121-how-gpu-nodes-are-provisioned">12.1 How GPU nodes are provisioned</a></h3>
<ul>
<li>Explains how GPU hardware, drivers, and runtimes come together as a usable node.</li>
<li>Shows why GPU nodes are not interchangeable with CPU nodes.</li>
<li>Establishes provisioning as the foundation for everything above it.</li>
</ul>
<h3 id="122-why-gpu-clusters-are-version-sensitive"><a class="header" href="#122-why-gpu-clusters-are-version-sensitive">12.2 Why GPU clusters are version-sensitive</a></h3>
<ul>
<li>Explains tight coupling between drivers, CUDA, runtimes, and workloads.</li>
<li>Shows how mismatches cause subtle or catastrophic failures.</li>
<li>Establishes version control as a stability requirement.</li>
</ul>
<h3 id="123-node-pools-upgrades-and-draining"><a class="header" href="#123-node-pools-upgrades-and-draining">12.3 Node pools, upgrades, and draining</a></h3>
<ul>
<li>Explains how GPU nodes are grouped and managed.</li>
<li>Shows why upgrades risk killing active workloads.</li>
<li>Establishes careful lifecycle management as mandatory.</li>
</ul>
<h3 id="124-capacity-expansion-vs-replacement"><a class="header" href="#124-capacity-expansion-vs-replacement">12.4 Capacity expansion vs replacement</a></h3>
<ul>
<li>Explains different ways to grow or refresh GPU capacity.</li>
<li>Shows trade-offs between risk, cost, and downtime.</li>
<li>Establishes planning as part of scaling.</li>
</ul>
<h3 id="125-why-just-upgrade-the-cluster-is-dangerous"><a class="header" href="#125-why-just-upgrade-the-cluster-is-dangerous">12.5 Why “just upgrade the cluster” is dangerous</a></h3>
<ul>
<li>Explains why GPU clusters resist in-place changes.</li>
<li>Shows how upgrades cascade across layers.</li>
<li>Establishes caution as an operational principle.</li>
</ul>
<h3 id="126-cloud-provider-constraints--gpu-reality"><a class="header" href="#126-cloud-provider-constraints--gpu-reality">12.6 Cloud Provider Constraints &amp; GPU Reality</a></h3>
<ul>
<li>Explains external limits imposed by cloud providers.</li>
<li>Shows why capacity planning is constrained by availability and quotas.</li>
<li>Establishes cloud constraints as a hard boundary on design.</li>
</ul>
<h3 id="127-activities"><a class="header" href="#127-activities">12.7 Activities</a></h3>
<ul>
<li>Explain why GPU clusters are more fragile than CPU clusters.</li>
<li>Analyze version coupling across drivers, runtimes, and workloads.</li>
<li>Design a safe GPU cluster upgrade strategy.</li>
<li>Simulate capacity expansion under cloud provider constraints.</li>
<li>Present: “Operating GPU Clusters in the Real World”</li>
</ul>
<hr>
<h2 id="section-13-experimentation-iteration--velocity"><a class="header" href="#section-13-experimentation-iteration--velocity">Section 13: Experimentation, Iteration &amp; Velocity</a></h2>
<h3 id="131-why-ai-infra-changes-more-frequently"><a class="header" href="#131-why-ai-infra-changes-more-frequently">13.1 Why AI infra changes more frequently</a></h3>
<ul>
<li>Explains why AI workloads evolve faster than traditional services.</li>
<li>Shows how infra must adapt to constant change.</li>
<li>Establishes flexibility as a core requirement.</li>
</ul>
<h3 id="132-config-churn-and-parameter-exploration"><a class="header" href="#132-config-churn-and-parameter-exploration">13.2 Config churn and parameter exploration</a></h3>
<ul>
<li>Explains frequent changes in batch sizes, precision, and runtime flags.</li>
<li>Shows how small config changes have large infra effects.</li>
<li>Establishes safe change management as critical.</li>
</ul>
<h3 id="133-fast-rollback-vs-slow-correctness"><a class="header" href="#133-fast-rollback-vs-slow-correctness">13.3 Fast rollback vs slow correctness</a></h3>
<ul>
<li>Explains why quick reversibility beats perfect first deployments.</li>
<li>Shows how rollback speed reduces blast radius.</li>
<li>Establishes rollback as a first-class capability.</li>
</ul>
<h3 id="134-supporting-many-experiments-safely"><a class="header" href="#134-supporting-many-experiments-safely">13.4 Supporting many experiments safely</a></h3>
<ul>
<li>Explains how concurrent experiments stress shared infra.</li>
<li>Shows why isolation and quotas matter.</li>
<li>Establishes guardrails for experimentation.</li>
</ul>
<h3 id="135-what-developer-velocity-means-for-ai-infra"><a class="header" href="#135-what-developer-velocity-means-for-ai-infra">13.5 What developer velocity means for AI infra</a></h3>
<ul>
<li>Explains velocity as time-to-feedback, not feature count.</li>
<li>Shows how infra can accelerate or block iteration.</li>
<li>Establishes infra as an enabler, not a bottleneck.</li>
</ul>
<h3 id="136-activities"><a class="header" href="#136-activities">13.6 Activities</a></h3>
<ul>
<li>Explain why AI infra evolves faster than traditional infra.</li>
<li>Analyze the impact of configuration churn on stability.</li>
<li>Design guardrails for safe experimentation at scale.</li>
<li>Evaluate rollback speed versus correctness trade-offs.</li>
<li>Present: “Supporting Velocity Without Chaos”</li>
</ul>
<hr>
<h2 id="section-14-cost-management--capacity-planning"><a class="header" href="#section-14-cost-management--capacity-planning">Section 14: Cost Management &amp; Capacity Planning</a></h2>
<h3 id="141-why-gpu-cost-dominates-everything"><a class="header" href="#141-why-gpu-cost-dominates-everything">14.1 Why GPU cost dominates everything</a></h3>
<ul>
<li>Explains the disproportionate cost of GPU resources.</li>
<li>Shows why inefficiency is immediately visible.</li>
<li>Establishes cost awareness as unavoidable.</li>
</ul>
<h3 id="142-idle-gpus-fragmentation-and-silent-waste"><a class="header" href="#142-idle-gpus-fragmentation-and-silent-waste">14.2 Idle GPUs, fragmentation, and silent waste</a></h3>
<ul>
<li>Explains common sources of unused GPU capacity.</li>
<li>Shows how waste hides behind “healthy” systems.</li>
<li>Establishes utilization as a cost signal.</li>
</ul>
<h3 id="143-spot-gpus-and-preemption-trade-offs"><a class="header" href="#143-spot-gpus-and-preemption-trade-offs">14.3 Spot GPUs and preemption trade-offs</a></h3>
<ul>
<li>Explains cost savings vs reliability risks.</li>
<li>Shows when preemption is acceptable.</li>
<li>Establishes policy-driven usage of spot capacity.</li>
</ul>
<h3 id="144-capacity-planning-for-launches-and-spikes"><a class="header" href="#144-capacity-planning-for-launches-and-spikes">14.4 Capacity planning for launches and spikes</a></h3>
<ul>
<li>Explains why GPU capacity cannot be spun up instantly.</li>
<li>Shows how demand spikes stress planning assumptions.</li>
<li>Establishes forecasting as an infra skill.</li>
</ul>
<h3 id="145-cost-signals-ai-infra-teams-must-track"><a class="header" href="#145-cost-signals-ai-infra-teams-must-track">14.5 Cost signals AI infra teams must track</a></h3>
<ul>
<li>Explains which metrics correlate with spend.</li>
<li>Shows how to attribute cost to workloads.</li>
<li>Establishes feedback loops between usage and cost.</li>
</ul>
<h3 id="146-activities"><a class="header" href="#146-activities">14.6 Activities</a></h3>
<ul>
<li>Explain why GPU inefficiency is immediately visible in cost.</li>
<li>Analyze hidden cost drivers like fragmentation and idle time.</li>
<li>Attribute cost to failed or inefficient workloads.</li>
<li>Redesign infra to reduce silent GPU burn.</li>
<li>Present: “Cost as a First-Class Signal”</li>
</ul>
<hr>
<h2 id="section-15-observability-for-ai-infrastructure"><a class="header" href="#section-15-observability-for-ai-infrastructure">Section 15: Observability for AI Infrastructure</a></h2>
<h3 id="151-why-cpu-metrics-are-insufficient-for-gpus"><a class="header" href="#151-why-cpu-metrics-are-insufficient-for-gpus">15.1 Why CPU metrics are insufficient for GPUs</a></h3>
<ul>
<li>Explains why traditional infra metrics fail to describe GPU behavior.</li>
<li>Shows how GPU bottlenecks remain invisible with CPU-only observability.</li>
<li>Establishes the need for GPU-aware signals.</li>
</ul>
<h3 id="152-gpu-level-metrics-that-actually-matter"><a class="header" href="#152-gpu-level-metrics-that-actually-matter">15.2 GPU-level metrics that actually matter</a></h3>
<ul>
<li>Explains which GPU metrics correlate with real performance.</li>
<li>Shows why many exposed metrics are misleading or incomplete.</li>
<li>Establishes metric selection as an infra design decision.</li>
</ul>
<h3 id="153-job-level-vs-system-level-visibility"><a class="header" href="#153-job-level-vs-system-level-visibility">15.3 Job-level vs system-level visibility</a></h3>
<ul>
<li>Explains the difference between per-workload and aggregate views.</li>
<li>Shows how lack of correlation obscures root causes.</li>
<li>Establishes multi-level visibility as necessary for debugging.</li>
</ul>
<h3 id="154-correlating-infra-model-and-performance-signals"><a class="header" href="#154-correlating-infra-model-and-performance-signals">15.4 Correlating infra, model, and performance signals</a></h3>
<ul>
<li>Explains why GPU metrics alone are insufficient.</li>
<li>Shows how infra state, model behavior, and performance interact.</li>
<li>Establishes correlation as the basis for diagnosis.</li>
</ul>
<h3 id="155-alerting-without-noise-in-gpu-systems"><a class="header" href="#155-alerting-without-noise-in-gpu-systems">15.5 Alerting without noise in GPU systems</a></h3>
<ul>
<li>Explains why naive alerting causes fatigue in GPU-heavy environments.</li>
<li>Shows how GPU behavior produces transient anomalies.</li>
<li>Establishes signal quality over alert quantity.</li>
</ul>
<h3 id="156-activities"><a class="header" href="#156-activities">15.6 Activities</a></h3>
<ul>
<li>Explain why GPU observability requires different signals than CPU systems.</li>
<li>Analyze misleading metrics and false alerts.</li>
<li>Correlate infra, workload, and cost metrics to diagnose issues.</li>
<li>Design alerting rules that avoid GPU noise.</li>
<li>Present: “Observability for GPU-Based Systems”</li>
</ul>
<hr>
<h2 id="section-16-production-operations--incident-response"><a class="header" href="#section-16-production-operations--incident-response">Section 16: Production Operations &amp; Incident Response</a></h2>
<h3 id="161-common-ai-infra-incidents"><a class="header" href="#161-common-ai-infra-incidents">16.1 Common AI infra incidents</a></h3>
<ul>
<li>Explains recurring failure patterns in production AI systems.</li>
<li>Shows how these differ from traditional service incidents.</li>
<li>Establishes incident familiarity as preparedness.</li>
</ul>
<h3 id="162-debugging-slow-stuck-or-failing-gpu-jobs"><a class="header" href="#162-debugging-slow-stuck-or-failing-gpu-jobs">16.2 Debugging slow, stuck, or failing GPU jobs</a></h3>
<ul>
<li>Explains systematic approaches to diagnosing runtime issues.</li>
<li>Shows how to distinguish infra faults from workload behavior.</li>
<li>Establishes structured debugging over guesswork.</li>
</ul>
<h3 id="163-safe-restarts-retries-and-recovery"><a class="header" href="#163-safe-restarts-retries-and-recovery">16.3 Safe restarts, retries, and recovery</a></h3>
<ul>
<li>Explains when recovery actions are safe or dangerous.</li>
<li>Shows how GPU workloads complicate restart semantics.</li>
<li>Establishes caution in automated recovery.</li>
</ul>
<h3 id="164-incident-response-when-gpus-burn-money"><a class="header" href="#164-incident-response-when-gpus-burn-money">16.4 Incident response when GPUs burn money</a></h3>
<ul>
<li>Explains why time-to-mitigation matters financially.</li>
<li>Shows how cost escalates during incidents.</li>
<li>Establishes cost-aware incident handling.</li>
</ul>
<h3 id="165-writing-postmortems-for-ai-failures"><a class="header" href="#165-writing-postmortems-for-ai-failures">16.5 Writing postmortems for AI failures</a></h3>
<ul>
<li>Explains why AI incidents require different analysis.</li>
<li>Shows how infra, workload, and cost intersect in failures.</li>
<li>Establishes learning and prevention as outcomes.</li>
</ul>
<h3 id="166-activities"><a class="header" href="#166-activities">16.6 Activities</a></h3>
<ul>
<li>Analyze common AI infra incidents and their root causes.</li>
<li>Debug a simulated slow or stuck GPU workload end-to-end.</li>
<li>Design safe recovery strategies under cost pressure.</li>
<li>Write a full postmortem covering infra, workload, and cost.</li>
<li>Present: “Operating AI Infrastructure Under Fire”</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="0003_intro-to-concurrent-programming-with-gpus.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="0003_intro-to-concurrent-programming-with-gpus.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
